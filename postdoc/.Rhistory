ytick_vals <= range_effects[2]
]
ytick_locs <-  ytick_vals / diff(range_effects) *  diff(range_yax) + range_yax[1]
points(x = effects_by_analyte$index,
y = effects_by_analyte$vloc,
col = effects_by_analyte$col,
pch = 19)
axis(4, at = ytick_locs, labels = ytick_vals, xpd = NA, las = 1)
text(x = pusr[2] + diff(pusr[1:2]) / 2, y = mean(pusr[3:4]),
labels = "magnitude of relative effect", srt = 270, xpd = NA)
# legend("topright",
#        legend = c("# non-zero folds", "Positive effect", "Negative effect"),
#        col = c("black", adjustcolor(c("blue", "red"), 0.75)),
#        lty = c(1, NA, NA),
#        lwd = c(3, NA, NA),
#        pch = c(NA, 19, 19),
#        pt.cex = 1, box.lty = 2)
#plot estimated coefficients
group <- c("molecular analytes",
"molecular PCs",
"phenotypes")[1]
nzc_sub <- as.matrix(all_nonzero_coefs[[group]])
nzc_sub_ncoef <- apply(nzc_sub, 2, function(nzc) sum(abs(nzc) > 1E-6))
min_ncoef <- 2
nzc_sub <- nzc_sub[,nzc_sub_ncoef >= min_ncoef]
nzc_sub_ncoef <- nzc_sub_ncoef[nzc_sub_ncoef >= min_ncoef]
nzcs_ord <- order(nzc_sub_ncoef, decreasing = T)
par(mar = c(7,5,3,5))
plot(nzc_sub_ncoef[nzcs_ord], type = "l",
ylab =  paste0("# non-zero estimates (/ ", nfolds, " folds)"),
xaxt = "n", yaxt = "n", xlab = "", lwd = 3,
main = paste0(tissue, ", ", ome, ", ", group))
axis(2, at = 1:nfolds, labels = 1:nfolds, las = 1)
n_feats <- ncol(nzc_sub)
abline(v = 1:n_feats, lty = 3, lwd = 0.75)
pusr <- par("usr")
segments(x0 = 1:n_feats, x1 = 1:n_feats,
y0 = pusr[3], y1 = pusr[3] - diff(pusr[3:4])/20, xpd = NA)
text(x = 1:n_feats + 0.1,
y = pusr[3] - diff(pusr[3:4])/15,
labels = names(nzc_sub_ncoef)[nzcs_ord], xpd = NA,
srt = 90, pos = 2, cex = 1)
cols <- adjustcolor(c("blue", "red"), 0.75)
effects_by_analyte <- do.call(rbind, lapply(1:n_feats, function(ai){
nz_vals <- nzc_sub[abs(nzc_sub[,ai]) > 1E-6, ai]
data.frame(abs_val = abs(nz_vals), col = cols[(sign(nz_vals) == 1) + 1],
index = nzcs_ord[ai])
}))
range_effects <- c(0, max(effects_by_analyte$abs_val))
range_yax <- pusr[3:4] + c(0.1, -0.1)
effects_by_analyte$vloc <- effects_by_analyte$abs_val /
diff(range_effects) *  diff(range_yax) + range_yax[1]
ytick_vals <- pretty(range_effects)
ytick_vals <- ytick_vals[ytick_vals >= range_effects[1] &
ytick_vals <= range_effects[2]
]
ytick_locs <-  ytick_vals / diff(range_effects) *  diff(range_yax) + range_yax[1]
points(x = effects_by_analyte$index,
y = effects_by_analyte$vloc,
col = effects_by_analyte$col,
pch = 19)
axis(4, at = ytick_locs, labels = ytick_vals, xpd = NA, las = 1)
text(x = pusr[2] + diff(pusr[1:2]) / 2, y = mean(pusr[3:4]),
labels = "magnitude of relative effect", srt = 270, xpd = NA)
# legend("topright",
# legend("topright",
#        legend = c("# non-zero folds", "Positive effect", "Negative effect"),
group <- c("molecular analytes",
"molecular PCs",
"phenotypes")[2]
nzc_sub <- as.matrix(all_nonzero_coefs[[group]])
nzc_sub_ncoef <- apply(nzc_sub, 2, function(nzc) sum(abs(nzc) > 1E-6))
min_ncoef <- 2
nzc_sub <- nzc_sub[,nzc_sub_ncoef >= min_ncoef]
nzc_sub_ncoef <- nzc_sub_ncoef[nzc_sub_ncoef >= min_ncoef]
nzcs_ord <- order(nzc_sub_ncoef, decreasing = T)
par(mar = c(7,5,3,5))
plot(nzc_sub_ncoef[nzcs_ord], type = "l",
ylab =  paste0("# non-zero estimates (/ ", nfolds, " folds)"),
xaxt = "n", yaxt = "n", xlab = "", lwd = 3,
main = paste0(tissue, ", ", ome, ", ", group))
axis(2, at = 1:nfolds, labels = 1:nfolds, las = 1)
n_feats <- ncol(nzc_sub)
abline(v = 1:n_feats, lty = 3, lwd = 0.75)
pusr <- par("usr")
segments(x0 = 1:n_feats, x1 = 1:n_feats,
y0 = pusr[3], y1 = pusr[3] - diff(pusr[3:4])/20, xpd = NA)
text(x = 1:n_feats + 0.1,
y = pusr[3] - diff(pusr[3:4])/15,
labels = names(nzc_sub_ncoef)[nzcs_ord], xpd = NA,
srt = 90, pos = 2, cex = 1)
cols <- adjustcolor(c("blue", "red"), 0.75)
effects_by_analyte <- do.call(rbind, lapply(1:n_feats, function(ai){
nz_vals <- nzc_sub[abs(nzc_sub[,ai]) > 1E-6, ai]
data.frame(abs_val = abs(nz_vals), col = cols[(sign(nz_vals) == 1) + 1],
index = nzcs_ord[ai])
}))
range_effects <- c(0, max(effects_by_analyte$abs_val))
range_yax <- pusr[3:4] + c(0.1, -0.1)
effects_by_analyte$vloc <- effects_by_analyte$abs_val /
diff(range_effects) *  diff(range_yax) + range_yax[1]
ytick_vals <- pretty(range_effects)
ytick_vals <- ytick_vals[ytick_vals >= range_effects[1] &
ytick_vals <= range_effects[2]
]
ytick_locs <-  ytick_vals / diff(range_effects) *  diff(range_yax) + range_yax[1]
points(x = effects_by_analyte$index,
y = effects_by_analyte$vloc,
col = effects_by_analyte$col,
pch = 19)
axis(4, at = ytick_locs, labels = ytick_vals, xpd = NA, las = 1)
text(x = pusr[2] + diff(pusr[1:2]) / 2, y = mean(pusr[3:4]),
labels = "magnitude of relative effect", srt = 270, xpd = NA)
# legend("topright",
#        legend = c("# non-zero folds", "Positive effect", "Negative effect"),
#        col = c("black", adjustcolor(c("blue", "red"), 0.75)),
#        lty = c(1, NA, NA),
#        lwd = c(3, NA, NA),
#        pch = c(NA, 19, 19),
#        pt.cex = 1, box.lty = 2)
group <- c("molecular analytes",
"molecular PCs",
"phenotypes")[3]
nzc_sub <- as.matrix(all_nonzero_coefs[[group]])
nzc_sub_ncoef <- apply(nzc_sub, 2, function(nzc) sum(abs(nzc) > 1E-6))
min_ncoef <- 2
nzc_sub <- nzc_sub[,nzc_sub_ncoef >= min_ncoef]
nzc_sub_ncoef <- nzc_sub_ncoef[nzc_sub_ncoef >= min_ncoef]
nzcs_ord <- order(nzc_sub_ncoef, decreasing = T)
par(mar = c(7,5,3,5))
plot(nzc_sub_ncoef[nzcs_ord], type = "l",
ylab =  paste0("# non-zero estimates (/ ", nfolds, " folds)"),
xaxt = "n", yaxt = "n", xlab = "", lwd = 3,
main = paste0(tissue, ", ", ome, ", ", group))
axis(2, at = 1:nfolds, labels = 1:nfolds, las = 1)
n_feats <- ncol(nzc_sub)
abline(v = 1:n_feats, lty = 3, lwd = 0.75)
pusr <- par("usr")
segments(x0 = 1:n_feats, x1 = 1:n_feats,
y0 = pusr[3], y1 = pusr[3] - diff(pusr[3:4])/20, xpd = NA)
text(x = 1:n_feats + 0.1,
y = pusr[3] - diff(pusr[3:4])/15,
labels = names(nzc_sub_ncoef)[nzcs_ord], xpd = NA,
srt = 90, pos = 2, cex = 1)
cols <- adjustcolor(c("blue", "red"), 0.75)
effects_by_analyte <- do.call(rbind, lapply(1:n_feats, function(ai){
nz_vals <- nzc_sub[abs(nzc_sub[,ai]) > 1E-6, ai]
data.frame(abs_val = abs(nz_vals), col = cols[(sign(nz_vals) == 1) + 1],
index = nzcs_ord[ai])
}))
range_effects <- c(0, max(effects_by_analyte$abs_val))
range_yax <- pusr[3:4] + c(0.1, -0.1)
effects_by_analyte$vloc <- effects_by_analyte$abs_val /
diff(range_effects) *  diff(range_yax) + range_yax[1]
ytick_vals <- pretty(range_effects)
ytick_vals <- ytick_vals[ytick_vals >= range_effects[1] &
ytick_vals <= range_effects[2]
]
ytick_locs <-  ytick_vals / diff(range_effects) *  diff(range_yax) + range_yax[1]
points(x = effects_by_analyte$index,
y = effects_by_analyte$vloc,
col = effects_by_analyte$col,
pch = 19)
axis(4, at = ytick_locs, labels = ytick_vals, xpd = NA, las = 1)
text(x = pusr[2] + diff(pusr[1:2]) / 2, y = mean(pusr[3:4]),
labels = "magnitude of relative effect", srt = 270, xpd = NA)
# legend("topright",
#        legend = c("# non-zero folds", "Positive effect", "Negative effect"),
#        col = c("black", adjustcolor(c("blue", "red"), 0.75)),
#        lty = c(1, NA, NA),
#        lwd = c(3, NA, NA),
#        pch = c(NA, 19, 19),
#        pt.cex = 1, box.lty = 2)
set.seed(123)
#### dependencies ####
#libraries
library(cmdstanr)
library(posterior)
library(dplyr)
library(data.table)
#functions
#note, normally I'd just source this file, but to streamline things I'll just copy
#over the relevant utility functions
# source("~/repos/Stan2R/R/functions.R")
subset_samps <- function(var_name, samps) {
colnames(samps) <- gsub("\\[|,", ".", colnames(samps))
colnames(samps) <- gsub("\\]", "", colnames(samps))
pattern <- paste0("^", var_name, "(\\.\\d+)*(\\.?)$")
matched_cols <- grep(pattern, names(samps), value = TRUE)
return(samps[, ..matched_cols])
}
munge_samps <- function(var_name, df) {
# Check if the data frame has only one row
if (nrow(df) == 1) {
# Process a single row to construct the appropriate data structure
# Determine the structure of the data (scalar, vector, matrix, or array)
col_names <- colnames(df)
col_names_without_var <- gsub(paste0("^", var_name, "\\."), "", col_names)
if (all(grepl("\\.\\d+\\.", col_names))) {
# Handle vectors, matrices, and arrays
# Extract indices from column names
indices <- lapply(strsplit(col_names_without_var, "\\."), function(x) as.numeric(x))
# Determine if it's a vector, matrix, or array
max_dims <- sapply(indices, length)
if (all(max_dims == 1)) {
# Vector
return(unlist(df))
} else if (all(max_dims == 2)) {
# Matrix
dim_order <- do.call(rbind, indices)
mat <- matrix(NA, nrow = max(dim_order[,1]), ncol = max(dim_order[,2]))
mat[cbind(dim_order[,1], dim_order[,2])] <- unlist(df)
return(mat)
} else {
# Array (higher dimensions)
array_dims <- apply(do.call(rbind, indices), 2, max)
arr <- array(NA, dim = array_dims)
array_indices <- do.call(expand.grid, lapply(array_dims, seq_len))
arr[as.matrix(array_indices)] <- unlist(df)
return(arr)
}
} else {
# Scalar or simple vector
return(unlist(df))
}
} else {
# Recursively apply to each row
return(lapply(seq_len(nrow(df)), function(i) munge_samps(var_name = var_name, df = df[i, , drop = FALSE])))
}
}
#set working directory to where the model files live
model_dir <- "~/scripts/minor_scripts/postdoc/"
setwd(model_dir)
#### simulate data  ####
#specify high level params & meta-params
check_mcmc_diag <- T
bad_tau0 <- F #should we parameterize the horseshoe prior model with a different expected # of hits?
lasso_col <- "orange" #color to draw the lasso pts
refit_stan_model <- T #should we refit the Stan model if we've already fitted it?
use_unilasso <- T #should we use the uniLasso or the Lasso?
unilasso.lower.limits <- 0 #what should the lower bound of the uniLasso be? (typically 0)
use_centered <- F #should we use the centered horseshow parameterization?
use_s2z <- F #should we use the sum-to-zero horseshoe paramterization?
ps_step <- F #should there be a post-selection step for the lasso?
rop <- 0.8 #what's the base the AR(1) correlation matrix (ie, raising rop^|i-j})
p <- 500 #how many total predictors should we have?
prop_n0_b <- 0.02 #what proportion of the true coefficients should be non-zero?
posterior_inclusion_threshold <- 0.75 #for feature selection, what should the horseshoe probability threshold look like?
B_pow <- 0 #we simulate non-null variables "B" from a normal distribution -- to what power should we raise them? (when B_pow = 0, they are all = 1)
Bn_mult <- 0 #we simulate non-null variables "B" from a normal distribution -- what should we multiply them by? (when Bn_mult = 0, they are all = 0)
invert_effects <- F #should we keep the AR(1) correlation structure, or invert some of the correlations?
error_prop_var <- 1 #what should the signal-to-noise ratio be? Relative to the epistemic variance, how much aleatoric variance should there be?
get_pmean_from_n0_samps <- F #should the posterior mean use all samples (=F), or just the ones that escape the spike (=T)?
n <- 300 #what should the sample size be?
#construct the correlation matrix
rs <- c(1, rop^(1:p))
R <- outer(1:p, 1:p, FUN = function(i, j, rs) rs[abs(i - j) + 1], rs = rs)
#sample inputs
x <- matrix(rnorm(n*p), n, p)
#discretize? to better reflect genetic context
x <- t(apply(x, 1, function(ri) rbinom(p, 2, prob = invlogit(ri))))
#heuristic prior spec for normal residuals
tau0 <- (p0/(p-p0)) * sd(y) / sqrt(n)
# regularized horseshoe prior params
p0 <- nB #parameterize horseshow w/ approx number of nonzero params
set.seed(123)
#### dependencies ####
#libraries
library(cmdstanr)
library(posterior)
library(dplyr)
library(data.table)
#functions
#note, normally I'd just source this file, but to streamline things I'll just copy
#over the relevant utility functions
# source("~/repos/Stan2R/R/functions.R")
subset_samps <- function(var_name, samps) {
colnames(samps) <- gsub("\\[|,", ".", colnames(samps))
colnames(samps) <- gsub("\\]", "", colnames(samps))
pattern <- paste0("^", var_name, "(\\.\\d+)*(\\.?)$")
matched_cols <- grep(pattern, names(samps), value = TRUE)
return(samps[, ..matched_cols])
}
munge_samps <- function(var_name, df) {
# Check if the data frame has only one row
if (nrow(df) == 1) {
# Process a single row to construct the appropriate data structure
# Determine the structure of the data (scalar, vector, matrix, or array)
col_names <- colnames(df)
col_names_without_var <- gsub(paste0("^", var_name, "\\."), "", col_names)
if (all(grepl("\\.\\d+\\.", col_names))) {
# Handle vectors, matrices, and arrays
# Extract indices from column names
indices <- lapply(strsplit(col_names_without_var, "\\."), function(x) as.numeric(x))
# Determine if it's a vector, matrix, or array
max_dims <- sapply(indices, length)
if (all(max_dims == 1)) {
# Vector
return(unlist(df))
} else if (all(max_dims == 2)) {
# Matrix
dim_order <- do.call(rbind, indices)
mat <- matrix(NA, nrow = max(dim_order[,1]), ncol = max(dim_order[,2]))
mat[cbind(dim_order[,1], dim_order[,2])] <- unlist(df)
return(mat)
} else {
# Array (higher dimensions)
array_dims <- apply(do.call(rbind, indices), 2, max)
arr <- array(NA, dim = array_dims)
array_indices <- do.call(expand.grid, lapply(array_dims, seq_len))
arr[as.matrix(array_indices)] <- unlist(df)
return(arr)
}
} else {
# Scalar or simple vector
return(unlist(df))
}
} else {
# Recursively apply to each row
return(lapply(seq_len(nrow(df)), function(i) munge_samps(var_name = var_name, df = df[i, , drop = FALSE])))
}
}
#set working directory to where the model files live
model_dir <- "~/scripts/minor_scripts/postdoc/"
setwd(model_dir)
#### simulate data  ####
#specify high level params & meta-params
check_mcmc_diag <- T
bad_tau0 <- F #should we parameterize the horseshoe prior model with a different expected # of hits?
lasso_col <- "orange" #color to draw the lasso pts
refit_stan_model <- T #should we refit the Stan model if we've already fitted it?
use_unilasso <- T #should we use the uniLasso or the Lasso?
unilasso.lower.limits <- 0 #what should the lower bound of the uniLasso be? (typically 0)
use_centered <- F #should we use the centered horseshow parameterization?
use_s2z <- F #should we use the sum-to-zero horseshoe paramterization?
ps_step <- F #should there be a post-selection step for the lasso?
rop <- 0.8 #what's the base the AR(1) correlation matrix (ie, raising rop^|i-j})
p <- 500 #how many total predictors should we have?
prop_n0_b <- 0.02 #what proportion of the true coefficients should be non-zero?
posterior_inclusion_threshold <- 0.75 #for feature selection, what should the horseshoe probability threshold look like?
B_pow <- 0 #we simulate non-null variables "B" from a normal distribution -- to what power should we raise them? (when B_pow = 0, they are all = 1)
Bn_mult <- 0 #we simulate non-null variables "B" from a normal distribution -- what should we multiply them by? (when Bn_mult = 0, they are all = 0)
invert_effects <- F #should we keep the AR(1) correlation structure, or invert some of the correlations?
error_prop_var <- 1 #what should the signal-to-noise ratio be? Relative to the epistemic variance, how much aleatoric variance should there be?
get_pmean_from_n0_samps <- F #should the posterior mean use all samples (=F), or just the ones that escape the spike (=T)?
n <- 300 #what should the sample size be?
#construct the correlation matrix
rs <- c(1, rop^(1:p))
R <- outer(1:p, 1:p, FUN = function(i, j, rs) rs[abs(i - j) + 1], rs = rs)
#sample inputs
x <- matrix(rnorm(n*p), n, p)
#discretize? to better reflect genetic context
x <- t(apply(x, 1, function(ri) rbinom(p, 2, prob = invlogit(ri))))
invlogit <- function(x) exp(x) / (1 + exp(x))
set.seed(123)
#### dependencies ####
#libraries
library(cmdstanr)
library(posterior)
library(dplyr)
library(data.table)
#functions
#note, normally I'd just source this file, but to streamline things I'll just copy
#over the relevant utility functions
# source("~/repos/Stan2R/R/functions.R")
invlogit <- function(x) exp(x) / (1 + exp(x))
subset_samps <- function(var_name, samps) {
colnames(samps) <- gsub("\\[|,", ".", colnames(samps))
colnames(samps) <- gsub("\\]", "", colnames(samps))
pattern <- paste0("^", var_name, "(\\.\\d+)*(\\.?)$")
matched_cols <- grep(pattern, names(samps), value = TRUE)
return(samps[, ..matched_cols])
}
munge_samps <- function(var_name, df) {
# Check if the data frame has only one row
if (nrow(df) == 1) {
# Process a single row to construct the appropriate data structure
# Determine the structure of the data (scalar, vector, matrix, or array)
col_names <- colnames(df)
col_names_without_var <- gsub(paste0("^", var_name, "\\."), "", col_names)
if (all(grepl("\\.\\d+\\.", col_names))) {
# Handle vectors, matrices, and arrays
# Extract indices from column names
indices <- lapply(strsplit(col_names_without_var, "\\."), function(x) as.numeric(x))
# Determine if it's a vector, matrix, or array
max_dims <- sapply(indices, length)
if (all(max_dims == 1)) {
# Vector
return(unlist(df))
} else if (all(max_dims == 2)) {
# Matrix
dim_order <- do.call(rbind, indices)
mat <- matrix(NA, nrow = max(dim_order[,1]), ncol = max(dim_order[,2]))
mat[cbind(dim_order[,1], dim_order[,2])] <- unlist(df)
return(mat)
} else {
# Array (higher dimensions)
array_dims <- apply(do.call(rbind, indices), 2, max)
arr <- array(NA, dim = array_dims)
array_indices <- do.call(expand.grid, lapply(array_dims, seq_len))
arr[as.matrix(array_indices)] <- unlist(df)
return(arr)
}
} else {
# Scalar or simple vector
return(unlist(df))
}
} else {
# Recursively apply to each row
return(lapply(seq_len(nrow(df)), function(i) munge_samps(var_name = var_name, df = df[i, , drop = FALSE])))
}
}
#set working directory to where the model files live
model_dir <- "~/scripts/minor_scripts/postdoc/"
setwd(model_dir)
#### simulate data  ####
#specify high level params & meta-params
check_mcmc_diag <- T
bad_tau0 <- F #should we parameterize the horseshoe prior model with a different expected # of hits?
lasso_col <- "orange" #color to draw the lasso pts
refit_stan_model <- T #should we refit the Stan model if we've already fitted it?
use_unilasso <- T #should we use the uniLasso or the Lasso?
unilasso.lower.limits <- 0 #what should the lower bound of the uniLasso be? (typically 0)
use_centered <- F #should we use the centered horseshow parameterization?
use_s2z <- F #should we use the sum-to-zero horseshoe paramterization?
ps_step <- F #should there be a post-selection step for the lasso?
rop <- 0.8 #what's the base the AR(1) correlation matrix (ie, raising rop^|i-j})
p <- 500 #how many total predictors should we have?
prop_n0_b <- 0.02 #what proportion of the true coefficients should be non-zero?
posterior_inclusion_threshold <- 0.75 #for feature selection, what should the horseshoe probability threshold look like?
B_pow <- 0 #we simulate non-null variables "B" from a normal distribution -- to what power should we raise them? (when B_pow = 0, they are all = 1)
Bn_mult <- 0 #we simulate non-null variables "B" from a normal distribution -- what should we multiply them by? (when Bn_mult = 0, they are all = 0)
invert_effects <- F #should we keep the AR(1) correlation structure, or invert some of the correlations?
error_prop_var <- 1 #what should the signal-to-noise ratio be? Relative to the epistemic variance, how much aleatoric variance should there be?
get_pmean_from_n0_samps <- F #should the posterior mean use all samples (=F), or just the ones that escape the spike (=T)?
n <- 300 #what should the sample size be?
#construct the correlation matrix
rs <- c(1, rop^(1:p))
R <- outer(1:p, 1:p, FUN = function(i, j, rs) rs[abs(i - j) + 1], rs = rs)
#sample inputs
x <- matrix(rnorm(n*p), n, p)
#discretize? to better reflect genetic context
x <- t(apply(x, 1, function(ri) rbinom(p, 2, prob = invlogit(ri))))
#then project to distribution of desired correlation matrix
if(n>p){
x <- x %*% solve(chol(cov(x))) %*% chol(R)
} else {
x <- x %*% chol(R) #whitening will just put 0s on the diag
}
#invert sign of x (to represent alternating correlation matrix)
if(invert_effects){
invertinds <- sample(1:p, floor(p/2), replace = F)
x[,invertinds] <- -x[,invertinds]
}
#simulate outcomes
nB <- ceiling(prop_n0_b * p)
nzi <- sample(1:p, nB, replace = F)
nnzi <- setdiff(1:p, nzi)
B <- qnorm(p = rbeta(nB, 0.2, 0.2))^B_pow
Bn <- qnorm(p = rbeta(p-nB, 50, 50))*Bn_mult
y <- apply(x[,nzi] %*% diag(B), 1, sum) +
apply(x[,nnzi] %*% diag(Bn), 1, sum)
e_sigma <- sd(y) * error_prop_var
e <- rnorm(n) * e_sigma
y <- y + e
#### marginal fits ####
#fit all params marginally
estimates <- do.call(rbind, apply(x, 2, function(xv)
summary(lm(y ~ xv))$coef[2,c(1,4)], simplify = F))
#### lasso fits ####
#fit the lasso or the unilasso
if (use_unilasso) {
library(uniLasso)
cv_fit   <- cv.uniLasso(x, y, nfolds = 10, lower.limits = unilasso.lower.limits)
est_coef <- as.numeric(coef(cv_fit, s = "lambda.min"))[-1]   # γ̂’s
} else {
lambda   <- glmnet::cv.glmnet(x, y)$lambda.min
fit      <- glmnet::glmnet(x, y, lambda = lambda, alpha = 1)
est_coef <- as.numeric(coef(fit))[-1]
}
nzi_est <- which(abs(est_coef) > 1E-5)      # selected variables
#refit according to a post-selection step
if (ps_step && length(nzi_est) > 0) {
alt_psf_fit <- summary(lm(y ~ x[, nzi_est]))$coef[-1, c(1, 4)]
coef_plot   <- alt_psf_fit[, 1]                 # OLS β̂
p_plot      <- alt_psf_fit[, 2]                 # p-values
} else {
coef_plot   <- est_coef[nzi_est]                # raw γ̂ (UniLasso) or β̂ (Lasso)
p_plot      <- rep(NA_real_, length(coef_plot)) # no p-values
}
#calculate point sizes for plotting
if (ps_step) {
# size ∝ –log10(p)
ps_cex <- -log10(p_plot)
ps_cex[is.infinite(ps_cex)] <- max(ps_cex[is.finite(ps_cex)])
ps_cex <- 1.5 + (ps_cex - min(ps_cex)) / (max(ps_cex) - min(ps_cex) + 1e-6) * 3
} else {
# size ∝ |coefficient|
mag     <- abs(coef_plot)
mag     <- abs(coef_plot) * 0 + 1
ps_cex  <- 1.5 + (mag - min(mag)) / (max(mag) - min(mag) + 1e-6) * 3
}
#### horseshoe fits  ####
#pack data into a list
# regularized horseshoe prior params
p0 <- nB #parameterize horseshow w/ approx number of nonzero params
pseudosigma <- sqrt(abs(1/mean(y)/(1-mean(y))))
#heuristic prior spec for logistic regression
tau0 <- p0/(p-p0)*pseudosigma/sqrt(n)
#heuristic prior spec for normal residuals
tau0 <- (p0/(p-p0)) * sd(y) / sqrt(n)
tau0
